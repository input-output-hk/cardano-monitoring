{
  self,
  inputs,
  moduleWithSystem,
  ...
}: {
  flake.nixosModules.common = moduleWithSystem ({system}: {
    name,
    pkgs,
    lib,
    config,
    ...
  }: {
    # Where colmena should deploy to, since we do not want to hardcode any IP
    # address, and not every host may have a DNS entry, we instead set this to
    # the same name as in the colmena configuration.
    #
    # Resolution of this name happens through our SSH_CONFIG_FILE, which is
    # generated by opentofu.
    deployment.targetHost = name;

    networking = {
      # Inherited from colmena.nix
      hostName = name;
      # Even though we use AWS security groups, another layer of firewall never
      # hurt anyone.
      firewall.enable = true;
    };

    # Ensures that all logging is done in UTC to make log aggregation easier.
    time.timeZone = "UTC";

    # By only supporting english, we reduce the size of our system closure a
    # bit. This comes at the cost of a higher initial build time since many
    # derivations depend on the locales.
    i18n.supportedLocales = ["en_US.UTF-8/UTF-8" "en_US/ISO-8859-1"];

    boot = {
      # On reboot, delete the contents of /tmp
      tmp.cleanOnBoot = true;

      # Mostly useful for debugging failed boots in the AWS console.
      kernelParams = ["boot.trace"];

      # Ensure /boot never gets too big, and we don't keep referencing old NixOS
      # versions forever.
      # Once removed from the Grub configuration, old NixOS versions will
      # automatically be garbage collected.
      loader.grub.configurationLimit = 5;
    };

    # Mostly utilities to setup and debug machines. Plus everyones favorite
    # editors.
    environment.systemPackages = with pkgs; [
      awscli2
      bat
      bind
      # Useful for the `growpart` utility, which can be used after resizing
      # a volume.
      cloud-utils
      di
      dnsutils
      fd
      fx
      file
      gitMinimal
      # More of a comprehensive dashboard than htop, but requires more resources
      # and provides no control
      glances
      helix
      # Easily see what's going on with a machine.
      htop
      ijq
      # Diff on steroids
      icdiff
      iptables
      jiq
      jq
      lsof
      neovim
      ncdu
      parted
      pciutils
      procps
      ripgrep
      rsync
      sops
      sysstat
      tcpdump
      tree
    ];

    sops = {
      # Ensure we always read and write things as they are, and not interpret them as YAML or JSON.
      defaultSopsFormat = "binary";

      # This token is required for auth-keys-hub. It requires at least org:read
      # permissions to check team membership.
      secrets.github-token = {
        sopsFile = "${self}/secrets/github-token.enc";
        owner = config.programs.auth-keys-hub.user;
        inherit (config.programs.auth-keys-hub) group;
      };
    };

    programs = {
      # Used by OpenSSH to allow logins with user keys as published on GitHub.
      auth-keys-hub = {
        enable = true;

        # This isn't in nixpkgs yet, so get it from the source.
        package = inputs.auth-keys-hub.packages.${system}.auth-keys-hub;

        # Persist the authorized_keys file across reboots.
        dataDir = "/var/lib/auth-keys-hub";

        github = {
          # We'll allow all members of these teams complete access via SSH.
          teams = ["input-output-hk/node-sre"];

          # For fallback purposes, we also specify some admins, since single
          # users don't require a GitHub token to update.
          users = ["manveru" "johnalotoski"];

          # The token from above.
          tokenFile = config.sops.secrets.github-token.path;
        };
      };

      # For ad-hoc terminal sharing we use tmux, start a session with `tmux`,
      # then other people join with `tmux attach`.
      tmux = {
        enable = true;
        aggressiveResize = true;
        clock24 = true;
        escapeTime = 0;
        historyLimit = 10000;
        newSession = true;
      };
    };

    services = {
      # For keeping our clock accurate, we use chrony.
      # There are a number of reasons why we prefer chrony over alternatives,
      # but generally it performs better within VMs and is more resilient to
      # changes in network and temperature.
      # https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/ch-configuring_ntp_using_the_chrony_suite#sect-differences_between_ntpd_and_chronyd
      chrony = {
        enable = true;

        # This will inform the kernel the system clock is kept synchronized and
        # the kernel will update the real-time clock every 11 minutes.
        extraConfig = "rtcsync";

        # This is not compatible with rtcsync, so disable it.
        enableRTCTrimming = false;
      };

      # Ensure this is disabled since we use chrony
      ntp.enable = false;

      # Very rarely things may depend on cron, or we have some ad-hoc scheduling
      # to do, so we enable it here.
      cron.enable = true;

      # Keep our log volume a little bit lower by automatically banning IPs
      # after 3 failed SSH login attempts. We don't allow password
      # authentication anyway, so those attempts are futile.
      # By default fail2ban includes a rule to ban for 10 minutes.
      # The drawback is that this does increase the SSH log volume by setting it
      # to VERBOSE.
      fail2ban.enable = true;

      # Our primary way to connect to and deploy machines.
      openssh = {
        enable = true;
        settings = {
          # We only allow private key authentication.
          PasswordAuthentication = false;

          # Ensure the RSA keys we allow are secure enough. Nobody should be
          # using 1024 bit keys in 2024.
          RequiredRSASize = 2048;

          # There's some controversy over the NIST curves, and they're not
          # particularly popular, so let's just avoid using them.
          PubkeyAcceptedAlgorithms = "-*nist*";
        };
      };

      # Metrics collection for this machine is handled by grafana-agent.
      # For fancier configuration we could also use Vector, but this is a bit
      # more convienent to configure and produces metrics that are compatible
      # with more existing dashboards.
      # The prometheus_remote_write option is specified in the monitoring module
      # to keep this configuration common even when we add machines that aren't
      # running Mimir.
      grafana-agent = {
        enable = true;

        # Don't phone home.
        extraFlags = ["-disable-reporting"];

        settings = {
          integrations = {
            # Since this is exclusively used on monitoring machines that are
            # running Mimir, we can simply push metrics to the local instance.
            prometheus_remote_write = [
              {url = "http://127.0.0.1:8080/mimir/api/v1/push";}
            ];

            # https://grafana.com/docs/agent/latest/static/configuration/integrations/node-exporter-config/#node_exporter_config
            node_exporter = {
              # A list of all kinds of metrics we wish to collect.
              # We include the defaults here for better visibility.
              enable_collectors = [
                "boottime"
                "cgroups"
                "conntrack"
                "cpu"
                "diskstats"
                "filefd"
                "filesystem"
                "interrupts"
                "lnstat"
                "loadavg"
                "logind"
                "meminfo"
                "netdev"
                "netstat"
                "network_route"
                "os"
                "perf"
                "processes"
                "qdisc"
                "sockstat"
                "softnet"
                "stat"
                "sysctl"
                "systemd"
                "time"
                "timex"
                "uname"
                "vmstat"
              ];

              # Disable default collectors we're not interested in.
              disable_collectors = [
                "btrfs"
                "infiniband"
                "nfs"
                "nfsd"
                "tapestats"
                "xfs"
                "zfs"
              ];
            };
          };

          # A list of targets to scrape metrics from.
          metrics = {
            configs = [
              {
                name = "integrations";
                remote_write = [{url = "http://127.0.0.1:8080/mimir/api/v1/push";}];
                scrape_configs = [
                  {
                    job_name = "blackbox";
                    metrics_path = "/probe";
                    params.module = ["https_2xx"];
                    scrape_interval = "1m";
                    static_configs = let
                      blackboxPath = (inputs."cardano-${name}") + "/flake/terraform/grafana/blackbox/blackbox.nix-import";
                    in
                      lib.mkIf (builtins.hasAttr "cardano-${name}" inputs && builtins.pathExists blackboxPath)
                      [
                        {
                          targets = import blackboxPath;
                        }
                      ];
                    relabel_configs = [
                      {
                        source_labels = ["__address__"];
                        target_label = "__param_target";
                      }
                      {
                        source_labels = ["__param_target"];
                        target_label = "instance";
                      }
                      {
                        replacement = "127.0.0.1:9115";
                        target_label = "__address__";
                      }
                    ];
                  }
                  {
                    job_name = "prometheus";
                    scheme = "http";
                    static_configs = [
                      {
                        targets = ["${config.services.prometheus.listenAddress}:${toString config.services.prometheus.port}"];
                      }
                    ];
                    metrics_path = "/metrics";
                  }
                  {
                    job_name = "mimir";
                    scheme = "http";
                    static_configs = [
                      {
                        targets = ["${config.services.mimir.configuration.server.http_listen_address}:${toString config.services.mimir.configuration.server.http_listen_port}"];
                      }
                    ];
                    metrics_path = "/mimir/metrics";
                  }
                  {
                    job_name = "grafana";
                    scheme = "http";
                    static_configs = [
                      {
                        targets = ["${config.services.grafana.settings.server.http_addr}:${toString config.services.grafana.settings.server.http_port}"];
                      }
                    ];
                    metrics_path = "/metrics";
                  }
                ];
              }
            ];
          };
        };
      };
    };

    system = {
      # Ensure the nixpgs repo is persisted on the machine itself for easier
      # debugging.
      extraSystemBuilderCmds = ''
        ln -sv ${pkgs.path} $out/nixpkgs
      '';

      # This provides a nice summary for all the things that change between deployments.
      # For example:
      # playground | [U.] #20 linux 6.1.64, 6.1.64-modules-shrunk -> 6.1.72, 6.1.72-modules-shrunk
      activationScripts.diff = {
        supportsDryActivation = true;
        text = ''
          ${pkgs.nvd}/bin/nvd --nix-bin-dir=${pkgs.nix}/bin diff /run/current-system "$systemConfig"
        '';
      };
    };

    nix = {
      # Ensure the default `nixpkgs` registry entry on the machine points to the
      # exact same version as this flake instead of nixpkgs-unstable.
      registry.nixpkgs.flake = inputs.nixpkgs;

      # Create hard links in the nix store to deduplicate files. This will run
      # every day at 03:45 UTC.
      optimise.automatic = true;

      # Run a garbage collection on the Nix store every day at 03:15 UTC.
      gc.automatic = true;

      settings = {
        # Optimise the store during normal Nix operations as well.
        auto-optimise-store = true;

        # Ensure we use binary caches for our downloads as much as possible.
        builders-use-substitutes = true;

        # For Nix 2.18, we still have to enable these flags to get the required
        # features.
        experimental-features = ["nix-command" "flakes"];

        # This option defines the maximum number of jobs that Nix will try to
        # build in parallel. Setting it to auto will allow running as many jobs
        # as there are CPU cores.
        max-jobs = "auto";

        # On errors, always show the full trace by default.
        show-trace = true;

        # We set our own Hydra as an additional binary cache.
        substituters = ["https://cache.iog.io"];

        # Avoid checking remote sources for updates if they were already updated
        # in the last 3 days.
        tarball-ttl = 60 * 60 * 72;

        # That's the public key for our Hydra, ignore the seemingly wrong host
        # name in there, we have been using the same key across a number of
        # migrations and unfortunately named the key after that host.
        trusted-public-keys = ["hydra.iohk.io:f/Ea+s+dFdN+3Y/G+FDgSq+a5NEWhJGzdjvKNGv0/EQ="];
      };
    };

    # Allow firmware updates for anything with a redistributable license.
    hardware.enableRedistributableFirmware = true;

    # This is the initially deployed NixOS version. Always keep this the same
    # unless there's a pressing reason to change it.
    # Changing this version will not actually update NixOS, but instead newer
    # versions of packages will be used that require manual migration.
    system.stateVersion = "23.05";
  });
}
